---
title: "CÃ³mo aprende una red neuronal (y cÃ³mo automatizar tareas en tu empresa)"
description: >-
    CÃ³mo funcionan las redes neuronales artificiales, el descenso de gradiente y la retropropagaciÃ³n. Aprende cÃ³mo entrenar una IA paso a paso y cÃ³mo aplicarla en la automatizaciÃ³n de tareas en tu negocio.
category: Machine Learning
pubDate: 2025-03-14
heroImage: "/images/como-aprende-red-neuronal.jpg"
tags:
  - Conceptos
  - Redes neuronales
  - Pyme
---


Â¿Sabes **quÃ© pasa realmente dentro de una red neuronal cuando "aprende"?** Hoy te lo explico con **ejemplos reales**, un **notebook prÃ¡ctico**, y la **teorÃ­a matemÃ¡tica** que hace que la magia ocurra.

Vamos a entrenar una **red neuronal sencilla** para clasificar correos de clientes en *quejas*, *preguntas* u *otros*. Pero no es solo cÃ³digo: quiero que entiendas el **proceso de aprendizaje paso a paso**, incluyendo cÃ³mo funciona el **descenso de gradiente** y la **retropropagaciÃ³n**.

---

## **1. Â¿QuÃ© es una red neuronal y cÃ³mo aprende?**

Piensa en una **red neuronal artificial** como un conjunto de **neuronas simuladas** que toman decisiones en cadena. Cada *neurona* recibe informaciÃ³n, la procesa y pasa el resultado a las siguientes.

**Â¿CÃ³mo aprende?**  
Ajustando los **pesos** de cada conexiÃ³n entre neuronas para que las decisiones sean mejores cada vez. Este ajuste se basa en **medir el error** (quÃ© tan lejos estamos del resultado correcto) y corregirlo poco a poco. AhÃ­ es donde entra el **descenso de gradiente** y la **retropropagaciÃ³n**.

---

## **2. El proceso de aprendizaje: teorÃ­a y prÃ¡ctica**

Te lo explico como si fuera un entrenamiento en tu empresa:

> **Piensa en un empleado nuevo** al que das tareas. Si se equivoca, le das feedback para que mejore. Cada vez que hace algo bien o mal, aprende y ajusta su forma de trabajar.  
En IA, las **redes neuronales** hacen lo mismo. Solo que el "feedback" es una operaciÃ³n matemÃ¡tica llamada **retropropagaciÃ³n**, y los "ajustes" se hacen con el **descenso de gradiente**.

---

## **3. Las fÃ³rmulas detrÃ¡s del aprendizaje**

### **3.1. Forward Pass: el recorrido hacia la predicciÃ³n**

Cada *neurona* hace dos cosas:
1. **Calcula una combinaciÃ³n lineal** de las entradas:
$$
z = w \cdot x + b
$$
- $w$: pesos
- $x$: entrada
- $b$: sesgo (bias)

2. **Aplica una funciÃ³n de activaciÃ³n** para decidir si pasa la informaciÃ³n:
$$
a = f(z)
$$
Donde $f$ puede ser, por ejemplo, una funciÃ³n **ReLU**:
$$
f(z) = \max(0, z)
$$

---

### **3.2. FunciÃ³n de pÃ©rdida (Loss Function): medir el error**
El modelo hace una predicciÃ³n. Â¿CÃ³mo sabemos si es buena?  
Usamos la **funciÃ³n de pÃ©rdida**. Para clasificaciÃ³n multiclase usamos **entropÃ­a cruzada**:
$$
L = - \sum_{i=1}^{C} y_i \cdot \log(\hat{y}_i)
$$
- $y_i$: valor real (0 o 1 segÃºn la clase correcta)
- $\hat{y}_i$: probabilidad que predijo el modelo para la clase $i$
- $C$: nÃºmero de clases (en nuestro caso, 3)

---

### **3.3. RetropropagaciÃ³n (Backpropagation): el feedback**

El **error** calculado vuelve hacia atrÃ¡s por la red para ajustar los pesos.

La **derivada de la pÃ©rdida** respecto a cada peso $w$ nos dice en quÃ© direcciÃ³n moverlo:
$$
\frac{\partial L}{\partial w}
$$

---

### **3.4. Descenso de gradiente (Gradient Descent): actualizar los pesos**

Una vez sabemos **cÃ³mo mover los pesos**, los ajustamos poco a poco:
$$
w = w - \eta \cdot \frac{\partial L}{\partial w}
$$
- $\eta$: tasa de aprendizaje (*learning rate*)
- $\frac{\partial L}{\partial w}$: gradiente (pendiente del error respecto al peso)

> **ImagÃ­nate bajar una montaÃ±a buscando el punto mÃ¡s bajo.** El gradiente te dice hacia dÃ³nde bajar, y $\eta$ es el tamaÃ±o del paso que das.

---

## **4. El notebook paso a paso (cÃ³digo + teorÃ­a)**

Ahora que sabes **quÃ© pasa por dentro**, veamos **cÃ³mo lo implementamos en Python**.
Para eso hemos hecho un **notebook interactivo**:  
 

<a href="https://colab.research.google.com/drive/1Oa8ok09oy4P6YFNlHvQzXuUma9mKD6x6?usp=sharing" target="_blank" rel="noopener">ðŸ‘‰ Abrir el notebook en Colab aquÃ­</a>


---

### **ðŸ“Œ Paso 1: Preparar el entorno**
Instalamos las librerÃ­as que usaremos:
```python
!pip install tensorflow nltk scikit-learn pandas matplotlib seaborn
```

---

### **ðŸ“Œ Paso 2: Dataset de correos**
Creamos un dataset simple con ejemplos de *quejas*, *preguntas* y *otros* correos.

```python
data = {
    "texto": [
        "No estoy satisfecho con el servicio, me cobraron de mÃ¡s.",
        "Â¿CuÃ¡nto cuesta el envÃ­o a Madrid?",
        "Gracias por la atenciÃ³n, todo estuvo excelente.",
        # ...
    ],
    "categoria": ["queja", "pregunta", "otro", ...]
}

df = pd.DataFrame(data)
```

> **Estos datos serÃ¡n nuestro "conjunto de entrenamiento"**. Es como el curso que damos a un nuevo empleado.

---

### **ðŸ“Œ Paso 3: Limpiar y preparar los datos**
Primero **limpiamos el texto**:
```python
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = set(stopwords.words("spanish"))

def limpiar_texto(texto):
    texto = texto.lower()
    texto = re.sub(r'[^\w\s]', '', texto)
    texto = " ".join([word for word in texto.split() if word not in stop_words])
    return texto

df["texto_limpio"] = df["texto"].apply(limpiar_texto)
```

Luego **tokenizamos y vectorizamos**:
```python
tokenizer = Tokenizer(num_words=1000, oov_token="<OOV>")
tokenizer.fit_on_texts(df["texto_limpio"])
sequences = tokenizer.texts_to_sequences(df["texto_limpio"])
padded = pad_sequences(sequences, padding="post")
```

Y **codificamos las etiquetas**:
```python
categorias = {"queja": 0, "pregunta": 1, "otro": 2}
df["categoria_num"] = df["categoria"].map(categorias)
```

---

### **ðŸ“Œ Paso 4: Dividir en entrenamiento y prueba**
Esto es clave para evitar que el modelo **memorice** en vez de aprender (overfitting).

```python
X_train, X_test, y_train, y_test = train_test_split(padded, df["categoria_num"], test_size=0.2, random_state=42)
```

---

### **ðŸ“Œ Paso 5: La red neuronal (Forward Pass)**

Nuestra red neuronal **aprende a travÃ©s de capas**:
```python
modelo = tf.keras.Sequential([
    tf.keras.layers.Embedding(1000, 16, input_length=padded.shape[1]),  # capa de embeddings
    tf.keras.layers.GlobalAveragePooling1D(),  # reduce la dimensionalidad
    tf.keras.layers.Dense(16, activation="relu"),  # capa oculta
    tf.keras.layers.Dense(3, activation="softmax")  # salida con probabilidades
])
```

**FunciÃ³n de pÃ©rdida**:
$$
L = - \sum_{i=1}^{C} y_i \cdot \log(\hat{y}_i)
$$

Compilamos:
```python
modelo.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
```

---

### **ðŸ“Œ Paso 6: Entrenamiento (Backward Pass y descenso de gradiente)**

```python
historial = modelo.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), verbose=2)
```

AquÃ­ ocurre el ciclo:
1. Predice â†’ calcula el error con **entropÃ­a cruzada**
2. Aplica **retropropagaciÃ³n**:
$$
\frac{\partial L}{\partial w}
$$
3. Ajusta los pesos con **descenso de gradiente**:
$$
w = w - \eta \cdot \frac{\partial L}{\partial w}
$$

---

### **ðŸ“Œ Paso 7: EvaluaciÃ³n del modelo**

Comprobamos si la red aprendiÃ³:
```python
y_pred = np.argmax(modelo.predict(X_test), axis=1)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred, labels=[0,1,2], target_names=categorias.keys()))
```

Y visualizamos la **matriz de confusiÃ³n**:
```python
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap="Blues", xticklabels=categorias.keys(), yticklabels=categorias.keys())
```

---

### **ðŸ“Œ Paso 8: Prueba en un caso real**
```python
def predecir_correo(texto):
    texto = limpiar_texto(texto)
    secuencia = tokenizer.texts_to_sequences([texto])
    secuencia_padded = pad_sequences(secuencia, maxlen=padded.shape[1])
    prediccion = modelo.predict(secuencia_padded)
    return list(categorias.keys())[np.argmax(prediccion)]

correo_nuevo = "Me llegÃ³ un producto daÃ±ado, Â¿quÃ© hago?"
print(predecir_correo(correo_nuevo))
```

---

### **ðŸ“Œ Paso 9: Visualizamos cÃ³mo aprendiÃ³**
```python
plt.plot(historial.history["accuracy"], label="PrecisiÃ³n en entrenamiento")
plt.plot(historial.history["val_accuracy"], label="PrecisiÃ³n en validaciÃ³n")
plt.xlabel("Ã‰pocas")
plt.ylabel("PrecisiÃ³n")
plt.legend()
plt.show()

plt.plot(historial.history["loss"], label="PÃ©rdida en entrenamiento")
plt.plot(historial.history["val_loss"], label="PÃ©rdida en validaciÃ³n")
plt.xlabel("Ã‰pocas")
plt.ylabel("PÃ©rdida")
plt.legend()
plt.show()
```

---

## **Resultado final**

Hemos visto **cÃ³mo se entrena una red neuronal**, tanto en la **teorÃ­a matemÃ¡tica** como en la **prÃ¡ctica de negocio**.

Hemos utilizado **aprendizaje supervisado**  
âœ… Entrenamiento con ejemplos  
âœ… Uso de **retropropagaciÃ³n** y **descenso de gradiente**  
âœ… Predicciones y mejoras con el tiempo

---

## **Â¿Quieres probarlo en tu empresa?**
ðŸ‘‰ Te puedo ayudar a implementar un modelo como este adaptado a tus **propios datos**.  
ðŸ‘‰ Agenda una llamada en [www.vialabsdigital.com](https://www.vialabsdigital.com) o responde a esta newsletter.  

---

**RaÃºl JÃ¡uregui**  
*Consultor en IA & Machine Learning*  
[Mindful ML](https://mindfulml.vialabsdigital.com)
