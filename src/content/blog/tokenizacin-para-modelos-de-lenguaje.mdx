---
title: Tokenizaci贸n para Modelos de Lenguaje
description: >-
  los tokenizadores BPE, WordPiece y SentencePiece transforman el texto para
  modelos de lenguaje. Para entender su funcionamiento, diferencias y
  aplicaciones pr谩cticas en NLP.
category: Inteligencia Artificial
pubDate: 2024-06-13T22:00:00.000Z
heroImage: /public/images/tokenizacion-llm.webp
tags:
  - LLM
---

# 隆Hola! 

La semana pasada te habl茅 sobre el fascinante mundo de los *[word embeddings](https://mindfulml.vialabsdigital.com/post/semantica-inteligencia-artificial/ "word embeddings")* y c贸mo estos representan las palabras en un espacio vectorial, ayud谩ndonos a entender las relaciones sem谩nticas entre ellas. Esta semana vamos a ver c贸mo se crean los *embeddings* partiendo de las palabras. Como vas a ver estos est谩n 铆ntimamente ligados a la tokenizaci贸n, un proceso crucial para cualquier modelo de lenguaje. As铆 que hoy, vamos a ver 驴qu茅 es la tokenizaci贸n?.

## Tokenizadores de LLMs

### 1. Los Tokenizadores de LLMs

La **tokenizaci贸n** es un proceso esencial en el procesamiento del lenguaje natural (*NLP*), ya que convierte el texto en una forma comprensible para los modelos de lenguaje. Los tokenizadores dividen el texto en palabras o subpalabras y asignan identificadores 煤nicos a cada token. Sin una buena tokenizaci贸n, los modelos no podr铆an interpretar correctamente el texto. Hay varias formas de hacer esto, sigue leyendo y lo vas a entender perfectamente.

#### Relaci贸n entre Tokenizaci贸n y Embeddings: Un Ejemplo Pr谩ctico

La imagen ilustra c贸mo una oraci贸n se procesa a trav茅s de un tokenizador y se convierte en *embeddings*, elementos fundamentales para que los modelos de lenguaje comprendan y generen texto.

![](public/images/tokenizacion_1.jpg)

Vamos a desglosar el proceso paso a paso:

> **Oraci贸n Original**: \
> "Today is a beautiful day outside."\
> **Tokenizaci贸n**: \
> La oraci贸n se divide en subpalabras o tokens: \
> \["To", "day", "is", "a", "beaut", "iful", "day", "out", "side", "."]

Aqu铆 se puede observar que algunas palabras se descomponen en subpalabras (por ejemplo, "beautiful" en "beaut" y "iful").

> **Asignaci贸n de Identificadores**:  \
> Cada token se convierte en un identificador 煤nico basado en un vocabulario predefinido:\
> \[98, 1452, 43, 15, 2932, 1709, 740, 1452, 3112, 3823, 74]

Estos identificadores son 铆ndices que corresponden a los embeddings almacenados en el modelo.

> **Generaci贸n de Embeddings**: \
> Cada identificador se convierte en un vector num茅rico de alta dimensi贸n, conocido como embedding, que captura las caracter铆sticas sem谩nticas del token correspondiente.

Estos embeddings permiten al modelo de lenguaje comprender el contexto y las relaciones entre las palabras en la oraci贸n.

La tokenizaci贸n y los *embeddings* por lo tanto trabajan juntos para transformar texto en datos que los modelos de lenguaje pueden interpretar y manipular eficazmente. Recuerda:

* **Tokenizaci贸n**: Es el primer paso crucial que convierte el texto en una forma que los modelos pueden procesar. Una buena tokenizaci贸n mejora la precisi贸n y eficiencia del modelo.
* **Embeddings**: Transforman los tokens en vectores que el modelo utiliza para comprender el significado y contexto, facilitando tareas como traducci贸n, generaci贸n de texto y an谩lisis de sentimiento.

### 2. Tokenizaci贸n por Emparejamiento de Bytes (Byte Pair Encoding - BPE)

#### Descripci贸n General

El *Byte Pair Encoding (BPE)* es un algoritmo de subtokenizaci贸n que fusiona los pares de bytes o caracteres consecutivos m谩s frecuentes en un corpus. Este m茅todo mejora la representaci贸n de las palabras, permitiendo una mayor flexibilidad y precisi贸n.

#### Pasos del proceso

* **Inicializaci贸n de la oraci贸n**: Dividimos la oraci贸n en palabras.
* **Creaci贸n del vocabulario inicial**: Empezamos con todos los caracteres individuales.
* **Iteraci贸n para fusionar caracteres frecuentes**: Fusionamos los pares de caracteres m谩s frecuentes.
* **Actualizaci贸n del vocabulario**: A帽adimos los nuevos pares fusionados al vocabulario.
* **Repetici贸n**: Continuamos el proceso hasta alcanzar el tama帽o de vocabulario deseado.

#### Ejemplo Pr谩ctico

Tomemos la frase "deep learning engineer":

1. Iniciamos con tokens individuales: \["d", "e", "e", "p", " ", "l", "e", "a", "r", "n", "i", "n", "g", " ", "e", "n", "g", "i", "n", "e", "e", "r"].
2. Fusionamos pares frecuentes como "d+e" -> "de".
3. Continuamos hasta obtener tokens m谩s largos y significativos.

### 3. Tokenizaci贸n por Pieza de Palabra (WordPiece)

#### Descripci贸n General

El *WordPiece* es similar al BPE, pero utiliza un criterio diferente para la fusi贸n de pares de s铆mbolos, maximizando la probabilidad de los datos de entrenamiento.

#### Proceso Detallado

* **Inicializaci贸n del vocabulario**: Comenzamos con todos los caracteres presentes en los datos.
* **C谩lculo de la probabilidad**: Evaluamos la probabilidad de los pares de tokens.
* **Fusi贸n de tokens**: Fusionamos el par que maximiza la probabilidad.
* **Comparaci贸n**: Contrastamos con el criterio de frecuencia del BPE.
* **Repetici贸n**: Continuamos hasta alcanzar el tama帽o de vocabulario deseado.

#### Ejemplo Pr谩ctico

Con la misma frase "deep learning engineer":

1. Iniciamos con tokens individuales.
2. Fusionamos tokens basados en la probabilidad de ocurrencia.
3. Comparamos los resultados con BPE para ver diferencias en los tokens generados.

### 4. Tokenizaci贸n por Pieza de Sentencia (SentencePiece)

#### Descripci贸n General

El *SentencePiece* est谩 dise帽ado para manejar lenguajes sin espacios expl铆citos entre palabras, como el japon茅s y el chino. Trata el texto como una secuencia de caracteres, incluyendo espacios.

#### Proceso Detallado

* **Uso del mismo algoritmo de fusi贸n**: Utilizamos BPE o el tokenizador unigram.
* **Inicializaci贸n del vocabulario**: Comenzamos con un gran n煤mero de tokens.
* **Reducci贸n progresiva**: Reducimos el vocabulario hasta el tama帽o deseado.

#### Ejemplo Pr谩ctico

Para "deep learning engineer":

1. Tratamos el texto como una secuencia continua.
2. Utilizamos guiones bajos como marcadores de espacio: "\_deep \_learning \_engineer".
3. Simplificamos la reconstrucci贸n de la oraci贸n original.

### 5. Comparaci贸n entre BPE, WordPiece y SentencePiece

|   |   |   |   |
| - | - | - | - |
|   |   |   |   |
|   |   |   |   |
|   |   |   |   |
|   |   |   |   |

As铆 que la elecci贸n del tokenizador adecuado puede marcar una gran diferencia en el rendimiento de tu modelo de lenguaje. Los m茅todos como BPE, WordPiece y SentencePiece tienen sus propias ventajas y desventajas, dependiendo del idioma y el contexto de uso. En el futuro, continuaremos viendo mejoras en estos algoritmos para facilitar a煤n m谩s el procesamiento del lenguaje natural.

Nos vemos la pr贸xima semana con m谩s novedades y aprendizajes sobre el fascinante mundo del Machine Learning y la Inteligencia Artificial.

隆Hasta la pr贸xima!

Saludos,
Ra煤l J谩uregui
