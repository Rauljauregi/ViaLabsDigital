---

title: "C√≥mo funciona realmente ChatGPT (y por qu√© solo usa un decodificador)"
description: "ChatGPT utiliza un modelo basado en la arquitectura Transformer, pero solo emplea la parte del decodificador. Aprende c√≥mo genera texto de manera autoregresiva y en qu√© se diferencia de otros modelos de IA."
category: Deep Learning
pubDate: "2025-02-17T18:00:00.000Z"
heroImage: '/images/como-funciona-chatgpt.jpg'
tags:
  - Transformers
  - Gpt
  - Caso pr√°ctico
---

## 1. **ChatGPT es m√°s que un modelo de IA**  

Cada vez que hablamos con ChatGPT, parece que "entiende" lo que decimos, que recuerda lo que hemos dicho antes e incluso que razona sobre sus respuestas. Pero ¬øc√≥mo lo hace realmente?  

La clave est√° en su arquitectura. ChatGPT es un modelo basado en **GPT (Generative Pre-trained Transformer)**, que a su vez se basa en la arquitectura **Transformer**.  

Sin embargo, a diferencia de otros modelos de IA como T5, que utilizan **codificadores y decodificadores**, **GPT solo usa la parte del decodificador**. ¬øPor qu√©? Porque est√° dise√±ado exclusivamente para **generar texto** y no para analizarlo en profundidad como lo har√≠a un buscador o una herramienta de an√°lisis de texto.  

Vamos a ver exactamente qu√© significa esto y c√≥mo influye en lo que hace ChatGPT.  


## 2. **La arquitectura detr√°s de ChatGPT**  

### **GPT y la arquitectura Transformer**  

GPT proviene de la arquitectura **Transformer**, introducida en el paper _"Attention Is All You Need"_ (2017). Esta arquitectura revolucion√≥ el procesamiento del lenguaje porque usa un mecanismo llamado **self-attention**, que le permite procesar texto de manera m√°s eficiente que los modelos anteriores.  

Ahora bien, un Transformer completo tiene dos partes:  

1. **El codificador (Encoder)**, que analiza el texto y lo comprende en ambas direcciones (de izquierda a derecha y de derecha a izquierda).  
2. **El decodificador (Decoder)**, que genera nuevo texto bas√°ndose en lo que ya ha procesado.  

GPT **solo usa la parte del decodificador**. Esto significa que no "lee" el texto en ambas direcciones, sino que **genera palabras de manera secuencial**, usando solo lo que ha visto antes en la conversaci√≥n, (para entender la uttilidad de esto hay que pensar en la fase de entrenamiento no en la de inferencia).  

¬øPor qu√© funciona as√≠? Porque su objetivo principal es **predecir la siguiente palabra** en funci√≥n del contexto previo.  

### **Diferencias entre un Transformer completo y GPT**  

| Caracter√≠stica           | Transformer Completo | GPT (Solo Decodificador) |
|-------------------------|----------------------|--------------------------|
| **Usa Codificador**     | ‚úÖ S√≠                 | ‚ùå No                    |
| **Usa Decodificador**   | ‚úÖ S√≠                 | ‚úÖ S√≠                    |
| **Comprensi√≥n del Contexto** | Bidireccional        | Unidireccional            |
| **Generaci√≥n de Texto** | Limitada              | ‚úÖ Optimizada para generaci√≥n |

GPT **sacrifica comprensi√≥n bidireccional** para ser m√°s eficiente generando texto.  


## 3. **¬øC√≥mo genera texto ChatGPT?**  

Lo que hace ChatGPT es bastante sencillo de describir (pero muy complejo de implementar):  

1. Toma el texto que le hemos dado como entrada.  
2. Predice cu√°l deber√≠a ser la siguiente palabra, bas√°ndose en el contexto previo.  
3. Repite el proceso una y otra vez hasta completar la respuesta.  

Este enfoque se llama **generaci√≥n autoregresiva**, porque cada palabra se basa en las anteriores.  

### **Self-attention causal: la clave de la generaci√≥n de texto**  

En un modelo Transformer tradicional, el **self-attention** le permite al modelo analizar todo el texto a la vez (<a href="https://mindfulml.vialabsdigital.com/post/que-son-los-transformers-en-la-ia/" target="_blank" rel="noopener">esto se explica en este art√≠culo sobre BERT</a>). Pero en GPT esto no es posible porque solo usa el decodificador.  

En su lugar, GPT en el entrenamiento usa un mecanismo llamado **self-attention causal**, que le impide mirar palabras futuras y solo le permite utilizar las palabras previas.  

Esto es lo que hace que ChatGPT parezca que "escribe" en tiempo real, palabra por palabra.  


## 4. **Juega con la arquitectura de ChatGPT**  

Para entender mejor c√≥mo funciona GPT, hemos hecho un **notebook interactivo** donde puedes:  

- Ver c√≥mo **predice las palabras** de manera autoregresiva.  
- Explorar la **matriz de atenci√≥n** y ver c√≥mo selecciona la informaci√≥n m√°s relevante en cada paso.  
- Analizar c√≥mo **procesa los tokens y embeddings** en tiempo real.  

<a href="https://colab.research.google.com/drive/1E4qFs0g0upxM5mZoHBK-TRosTFvrvmHu?usp=sharing" target="_blank" rel="noopener">üëâ Abrir el notebook en Colab aqu√≠</a>

### 4.1. **Generaci√≥n de Historias con GPT-2**
En el notebook, usamos el modelo DeepESP/gpt2-spanish para generar historias. Esto nos permite experimentar directamente con la estructura del decoder en acci√≥n:

- Ingresamos una frase inicial, y pulsamos ***enter** en tu teclado.
- GPT-2 genera la continuaci√≥n de la historia usando el contexto previo.
- Podemos seguir a√±adiendo frases a la ventana de contexto o escribir **salir**.

Cuando has introducido **salir**, visualizar√°s la matriz de atenci√≥n. Esta matriz muestra:
Esta matriz muestra:

- Qu√© tokens (palabras) est√°n influenciando a otros tokens.
- Cu√°nta "atenci√≥n" da el modelo a cada palabra previa.
- C√≥mo la atenci√≥n cambia en cada capa del decodificador.

En t√©rminos simples, la matriz de atenci√≥n indica cu√°les palabras son m√°s importantes en el contexto para predecir la siguiente.

Al probar diferentes entradas y analizar las matrices de atenci√≥n, podemos ver c√≥mo el modelo asigna pesos a cada palabra seg√∫n su relevancia en la oraci√≥n generada.

### 4.2.  **La Matriz de Atenci√≥n en Nuestro Notebook**  

En nuestro **notebook**, la **matriz de atenci√≥n** es la representaci√≥n visual de **c√≥mo el modelo asigna pesos a las palabras previas para generar la siguiente palabra**.  

Cuando ejecutamos la funci√≥n `visualizar_atencion(input_texto)`, extraemos la √∫ltima matriz de atenci√≥n del modelo **GPT-2 en espa√±ol** y la graficamos con *seaborn*. Esta matriz muestra:  

- **Qu√© tokens (palabras) est√°n influenciando a otros tokens.**  
- **Cu√°nta "atenci√≥n" da el modelo a cada palabra previa.**  
- **C√≥mo la atenci√≥n cambia en cada capa del decodificador.**  

En t√©rminos simples, la matriz de atenci√≥n indica **cu√°les palabras son m√°s importantes en el contexto** para predecir la siguiente.  


### 4.3. **Diferencia con Multi-Head Self-Attention en BERT**  

Ya que hemos trabajado con **BERT**, sabemos que usa **multi-head self-attention** en su codificador. GPT tambi√©n usa **multi-head self-attention**, pero hay diferencias clave:  

| Caracter√≠stica                  | BERT (Encoder) | GPT (Decoder-Only) |
|----------------------------------|---------------|--------------------|
| **Direccionalidad**             | Bidireccional | Unidireccional (causal) |
| **Self-Attention Mask**         | Ninguna (puede ver todo el texto) | Causal mask (bloquea tokens futuros) |
| **Flujo de Atenci√≥n**           | Cada token puede atender a cualquier otro (izq. y der.) | Cada token solo atiende a los anteriores |
| **Uso del Multi-Head Attention** | Codifica contexto en profundidad | Predice texto paso a paso |

üîπ **En BERT**, cada palabra puede atender a todas las dem√°s, porque su objetivo es comprender el significado de una oraci√≥n completa.  
üîπ **En GPT**, cada palabra solo puede atender a las anteriores, porque su objetivo es generar texto de manera autoregresiva.  

Cuando visualizamos la matriz de atenci√≥n en el **notebook**, vemos que cada token solo est√° influenciado por los tokens anteriores, debido a la **self-attention causal** implementada con una **m√°scara triangular inferior**.  



### 4.4. **Ejemplo Visual con la Matriz de Atenci√≥n**  

Si introducimos este texto en el **notebook**:  

> "El gato se subi√≥ al √°rbol"  

La matriz de atenci√≥n de la √∫ltima capa puede verse as√≠:  

```python
attention_mask = [
    [1, 0, 0, 0, 0, 0],
    [1, 1, 0, 0, 0, 0],
    [1, 1, 1, 0, 0, 0],
    [1, 1, 1, 1, 0, 0],
    [1, 1, 1, 1, 1, 0],
    [1, 1, 1, 1, 1, 1]
]
```

Cada fila representa c√≥mo un token (palabra) se relaciona con los anteriores. **El token "√°rbol" no puede atender a "subi√≥"**, porque eso romper√≠a la generaci√≥n autoregresiva.  

En cambio, si vi√©ramos la matriz de atenci√≥n en **BERT**, todos los tokens podr√≠an atender a todos los dem√°s, porque su objetivo es comprensi√≥n, no generaci√≥n.  


## 5. **Preguntas Frecuentes**  

### **¬øC√≥mo funciona el algoritmo de ChatGPT?**  
ChatGPT usa un **modelo GPT**, que genera texto **prediciendo la siguiente palabra** en funci√≥n del contexto previo.  

### **¬øC√≥mo saber si un texto o c√≥digo ha sido generado con ChatGPT?**  
Hay herramientas que intentan detectar patrones en los textos generados por IA, pero todav√≠a no son 100% precisas.  

### **¬øQu√© diferencia hay entre GPT-3 y GPT-4?**  
GPT-4 tiene una arquitectura m√°s eficiente, maneja mejor el contexto y puede aceptar entradas m√°s largas.  

### **¬øTodos los modelos de IA generan texto como ChatGPT?**  
No. Algunos modelos como <a href="https://mindfulml.vialabsdigital.com/post/que-son-los-transformers-en-la-ia/" target="_blank" rel="noopener">BERT</a> est√°n dise√±ados para comprender texto en lugar de generarlo.  



ChatGPT funciona porque usa un **modelo solo de decodificaci√≥n**, basado en la arquitectura Transformer. Esto lo hace muy eficiente para **generar texto**, pero tambi√©n tiene limitaciones en la comprensi√≥n profunda del lenguaje.  

Si te interesa entender mejor c√≥mo funciona todo esto y c√≥mo aplicarlo a tu negocio, te puedo ayudar <a href="https://vialabsdigital.com/contacto/" target="_blank" rel="noopener">pulsa aqu√≠</a> . 

Adem√°s te puedes suscribir a la newsletter y cada viernes te env√≠o informaci√≥n pr√°ctica sobre Machine Learning e IA aplicada a empresas.  

