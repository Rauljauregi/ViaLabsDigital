---

title: "C√≥mo funciona realmente ChatGPT (y por qu√© solo usa un decodificador)"
description: "ChatGPT utiliza un modelo basado en la arquitectura Transformer, pero solo emplea la parte del decodificador. Aprende c√≥mo genera texto de manera autoregresiva y en qu√© se diferencia de otros modelos de IA."
category: Deep Learning
pubDate: "2025-02-17T18:00:00.000Z"
heroImage: '/images/como-funciona-chatgpt.jpg'
tags:
  - Transformers
  - Gpt
  - Caso pr√°ctico
---


## 1. **ChatGPT y la revoluci√≥n de los modelos Decoder-Only**  

Cuando interactuamos con ChatGPT, parece que **entiende lo que decimos**, recuerda el contexto de la conversaci√≥n y genera respuestas coherentes. Pero detr√°s de esta capacidad no hay comprensi√≥n real, sino un modelo basado en patrones estad√≠sticos: **GPT (Generative Pre-trained Transformer)**.  

GPT se basa en la arquitectura **Transformer**, introducida en el paper <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">*"Attention Is All You Need"* (2017)</a>. Sin embargo, el modelo **no usa un Transformer completo** (que incluye un **codificador** y un **decodificador**), sino que **solo emplea la parte del decodificador**.  

### **¬øPor qu√© GPT solo usa el decodificador?**  
La idea de usar **solo el decodificador** en modelos generativos fue introducida por OpenAI en 2018 con el paper <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">*"Improving Language Understanding by Generative Pre-Training"* (Radford et al., 2018)</a>.  

Este trabajo revolucion√≥ el aprendizaje del lenguaje al demostrar que un modelo pod√≠a entrenarse en grandes vol√∫menes de texto sin etiquetas (pre-entrenamiento) y luego ajustarse con ejemplos espec√≠ficos (fine-tuning) para mejorar su rendimiento en tareas concretas.  

La clave de este enfoque es que **el modelo aprende a predecir la siguiente palabra** en una secuencia, sin necesidad de comprender el texto en ambas direcciones como lo har√≠a un codificador.  

Esto tiene varias ventajas:  
‚úÖ **Es m√°s eficiente para generar texto**, ya que el modelo solo necesita conocer el contexto previo.  
‚úÖ **Se adapta mejor a la generaci√≥n autoregresiva**, porque cada token generado se convierte en parte de la entrada para el siguiente paso.  
‚úÖ **Es escalable**, permitiendo entrenar modelos m√°s grandes con menos restricciones de memoria que un Transformer completo.  

### **¬øC√≥mo influye en el comportamiento de ChatGPT?**  
El uso exclusivo del decodificador implica que **ChatGPT no analiza el significado del texto en profundidad**, sino que simplemente **predice la palabra m√°s probable que debe venir a continuaci√≥n**.  

Este mecanismo hace que:  
- **Genere respuestas fluidas y coherentes**, ya que sigue patrones de lenguaje natural.  
- **Sea capaz de mantener un contexto en la conversaci√≥n**, aunque no con una memoria real, sino recordando los tokens anteriores dentro de su ventana de contexto.  
- **No tenga una comprensi√≥n real del mundo**, pues solo basa sus respuestas en correlaciones aprendidas en los datos de entrenamiento.  

### **La diferencia clave con otros modelos**  
GPT se diferencia de otros modelos como **T5 o BERT**, que utilizan **codificadores** para comprender el lenguaje en ambas direcciones. Mientras que T5 es un modelo **encoder-decoder** y <a href="https://mindfulml.vialabsdigital.com/post/que-son-los-transformers-en-la-ia/" target="_blank" rel="noopener">**BERT**</a> usa **solo codificador**, GPT se basa √∫nicamente en el **decodificador**, optimizando su arquitectura para la generaci√≥n de texto. ** ChatGPT **es un modelo de lenguaje generativo basado solo en el decodificador de un Transformer**, una idea que se estableci√≥ en 2018 y ha evolucionado hasta modelos m√°s avanzados como GPT-4.  


## 2. **La arquitectura detr√°s de ChatGPT**  

### **GPT y la arquitectura Transformer**  

GPT proviene de la arquitectura **Transformer**, introducida en el paper _"Attention Is All You Need"_ (2017). Esta arquitectura revolucion√≥ el procesamiento del lenguaje porque usa un mecanismo llamado **self-attention**, que le permite procesar texto de manera m√°s eficiente que los modelos anteriores.  

Ahora bien, un Transformer completo tiene dos partes:  

1. **El codificador (Encoder)**, que analiza el texto y lo comprende en ambas direcciones (de izquierda a derecha y de derecha a izquierda).  
2. **El decodificador (Decoder)**, que genera nuevo texto bas√°ndose en lo que ya ha procesado.  

GPT **solo usa la parte del decodificador**. Esto significa que no "lee" el texto en ambas direcciones, sino que **genera palabras de manera secuencial**, usando solo lo que ha visto antes en la conversaci√≥n, (para entender la uttilidad de esto hay que pensar en la fase de entrenamiento no en la de inferencia).  

¬øPor qu√© funciona as√≠? Porque su objetivo principal es **predecir la siguiente palabra** en funci√≥n del contexto previo.  

### **Diferencias entre un Transformer completo y GPT**  

| Caracter√≠stica           | Transformer Completo | GPT (Solo Decodificador) |
|-------------------------|----------------------|--------------------------|
| **Usa Codificador**     | ‚úÖ S√≠                 | ‚ùå No                    |
| **Usa Decodificador**   | ‚úÖ S√≠                 | ‚úÖ S√≠                    |
| **Comprensi√≥n del Contexto** | Bidireccional        | Unidireccional            |
| **Generaci√≥n de Texto** | Limitada              | ‚úÖ Optimizada para generaci√≥n |

GPT **sacrifica comprensi√≥n bidireccional** para ser m√°s eficiente generando texto.  


## 3. **¬øC√≥mo genera texto ChatGPT?**  

Lo que hace ChatGPT es bastante sencillo de describir (pero muy complejo de implementar):  

1. Toma el texto que le hemos dado como entrada.  
2. Predice cu√°l deber√≠a ser la siguiente palabra, bas√°ndose en el contexto previo.  
3. Repite el proceso una y otra vez hasta completar la respuesta.  

Este enfoque se llama **generaci√≥n autoregresiva**, porque cada palabra se basa en las anteriores.  

### **Self-attention causal: la clave de la generaci√≥n de texto**  

En un modelo Transformer tradicional, el **self-attention** le permite al modelo analizar todo el texto a la vez (<a href="https://mindfulml.vialabsdigital.com/post/que-son-los-transformers-en-la-ia/" target="_blank" rel="noopener">esto se explica en este art√≠culo sobre BERT</a>). Pero en GPT esto no es posible porque solo usa el decodificador.  

En su lugar, GPT en el entrenamiento usa un mecanismo llamado **self-attention causal**, que le impide mirar palabras futuras y solo le permite utilizar las palabras previas.  

Esto es lo que hace que ChatGPT parezca que "escribe" en tiempo real, palabra por palabra.  


## 4. **Juega con la arquitectura de ChatGPT**  

Para entender mejor c√≥mo funciona GPT, hemos hecho un **notebook interactivo** donde puedes:  

- Ver c√≥mo **predice las palabras** de manera autoregresiva.  
- Explorar la **matriz de atenci√≥n** y ver c√≥mo selecciona la informaci√≥n m√°s relevante en cada paso.  
- Analizar c√≥mo **procesa los tokens y embeddings** en tiempo real.  

<a href="https://colab.research.google.com/drive/1E4qFs0g0upxM5mZoHBK-TRosTFvrvmHu?usp=sharing" target="_blank" rel="noopener">üëâ Abrir el notebook en Colab aqu√≠</a>

### 4.1. **Generaci√≥n de Historias con GPT-2**
En el notebook, usamos el modelo DeepESP/gpt2-spanish para generar historias. Esto nos permite experimentar directamente con la estructura del decoder en acci√≥n:

- Ingresa una frase inicial, y pulsa ***enter** en tu teclado.
- GPT-2 genera la continuaci√≥n de la historia usando el contexto previo.
- Puedes seguir a√±adiendo frases a la ventana de contexto o escribir **salir**.

Cuando has introducido **salir**, visualizar√°s la matriz de atenci√≥n. Esta matriz muestra:
Esta matriz muestra:

- Qu√© tokens (palabras) est√°n influenciando a otros tokens.
- Cu√°nta "atenci√≥n" da el modelo a cada palabra previa.
- C√≥mo la atenci√≥n cambia en cada capa del decodificador.

En t√©rminos simples, la matriz de atenci√≥n indica qu√© palabras son m√°s importantes en el contexto para predecir la siguiente.

Al probar diferentes entradas y analizar las matrices de atenci√≥n, podemos ver c√≥mo el modelo asigna pesos a cada palabra seg√∫n su relevancia en la oraci√≥n generada.

### 4.2.  **La Matriz de Atenci√≥n en Nuestro Notebook**  

En nuestro **notebook**, la **matriz de atenci√≥n** es la representaci√≥n visual de **c√≥mo el modelo asigna pesos a las palabras previas para generar la siguiente palabra**.  

Cuando ejecutamos la funci√≥n `visualizar_atencion(input_texto)`, extraemos la √∫ltima matriz de atenci√≥n del modelo **GPT-2 en espa√±ol** y la graficamos con *seaborn*. Esta matriz muestra:  

- **Qu√© tokens (palabras) est√°n influenciando a otros tokens.**  
- **Cu√°nta "atenci√≥n" da el modelo a cada palabra previa.**  
- **C√≥mo la atenci√≥n cambia en cada capa del decodificador.**  

En t√©rminos simples, la matriz de atenci√≥n indica **cu√°les palabras son m√°s importantes en el contexto** para predecir la siguiente.  


### 4.3. **Diferencia con Multi-Head Self-Attention en BERT**  

Ya explicamos en el art√≠culo como trabajaba <a href="https://mindfulml.vialabsdigital.com/post/que-son-los-transformers-en-la-ia/" target="_blank" rel="noopener">BERT</a> , y all√≠ se explica el **multi-head self-attention** en el codificador. GPT tambi√©n usa **multi-head self-attention**, pero hay diferencias muy importantes:  

| Caracter√≠stica                  | BERT (Encoder) | GPT (Decoder-Only) |
|----------------------------------|---------------|--------------------|
| **Direccionalidad**             | Bidireccional | Unidireccional (causal) |
| **Self-Attention Mask**         | Ninguna (puede ver todo el texto) | Causal mask (bloquea tokens futuros) |
| **Flujo de Atenci√≥n**           | Cada token puede atender a cualquier otro (izq. y der.) | Cada token solo atiende a los anteriores |
| **Uso del Multi-Head Attention** | Codifica contexto en profundidad | Predice texto paso a paso |

üîπ **En BERT**, cada palabra puede atender a todas las dem√°s, porque su objetivo es comprender el significado de una oraci√≥n completa.  
üîπ **En GPT**, cada palabra solo puede atender a las anteriores, porque su objetivo es generar texto de manera autoregresiva.  

Cuando visualizamos la matriz de atenci√≥n en el **notebook**, vemos que cada token solo est√° influenciado por los tokens anteriores, debido a la **self-attention causal** implementada con una **m√°scara triangular inferior**.  



### 4.4. **Ejemplo Visual con la Matriz de Atenci√≥n**  

Si introducimos este texto en el **notebook**:  

> "El gato se subi√≥ al √°rbol"  

La matriz de atenci√≥n de la √∫ltima capa puede verse as√≠:  

```python
attention_mask = [
    [1, 0, 0, 0, 0, 0],
    [1, 1, 0, 0, 0, 0],
    [1, 1, 1, 0, 0, 0],
    [1, 1, 1, 1, 0, 0],
    [1, 1, 1, 1, 1, 0],
    [1, 1, 1, 1, 1, 1]
]
```

Cada fila representa c√≥mo un token (palabra) se relaciona con los anteriores. **El token "√°rbol" no puede atender a "subi√≥"**, porque eso romper√≠a la generaci√≥n autoregresiva.  

En cambio, si vi√©ramos la matriz de atenci√≥n en **BERT**, todos los tokens podr√≠an atender a todos los dem√°s, porque su objetivo es comprensi√≥n, no generaci√≥n.  


## 5. **Preguntas Frecuentes**  

### **¬øC√≥mo funciona el algoritmo de ChatGPT?**  
ChatGPT usa un **modelo GPT**, que genera texto **prediciendo la siguiente palabra** en funci√≥n del contexto previo.  

### **¬øC√≥mo saber si un texto o c√≥digo ha sido generado con ChatGPT?**  
Hay herramientas que intentan detectar patrones en los textos generados por IA, pero todav√≠a no son 100% precisas.  

### **¬øQu√© diferencia hay entre GPT-3 y GPT-4?**  
GPT-4 tiene una arquitectura m√°s eficiente, maneja mejor el contexto y puede aceptar entradas m√°s largas.  

### **¬øTodos los modelos de IA generan texto como ChatGPT?**  
No. Algunos modelos como <a href="https://mindfulml.vialabsdigital.com/post/que-son-los-transformers-en-la-ia/" target="_blank" rel="noopener">BERT</a> est√°n dise√±ados para comprender texto en lugar de generarlo.  



ChatGPT funciona porque usa un **modelo solo de decodificaci√≥n**, basado en la arquitectura Transformer. Esto lo hace muy eficiente para **generar texto**, pero tambi√©n tiene limitaciones en la comprensi√≥n profunda del lenguaje.  

Si te interesa entender mejor c√≥mo funciona todo esto y c√≥mo aplicarlo a tu negocio, te puedo ayudar <a href="https://vialabsdigital.com/contacto/" target="_blank" rel="noopener">pulsa aqu√≠</a>. 

Adem√°s te puedes suscribir a la newsletter y cada viernes te env√≠o informaci√≥n pr√°ctica sobre Machine Learning e IA aplicada a empresas.  

