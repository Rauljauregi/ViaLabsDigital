---
title: "QuÃ© tiene de especial DeepSeek"
description: >-
    DeepSeek revoluciona la IA con menos consumo energÃ©tico, arquitectura Mixture of Experts y modelos multimodales. CÃ³mo impactarÃ¡ en el mundo de la IA.
category: Deep Learning
pubDate: 2025-03-23
heroImage: "/images/deepseek.jpeg"
tags:
  - Transformers
  - Redes neuronales
  - llm
---


> Si piensas que ya lo has visto todo en inteligencia artificial, prepÃ¡rate: **DeepSeek** acaba de subir el listÃ³n. Y no solo promete modelos mÃ¡s potentes, sino tambiÃ©n **mÃ¡s rÃ¡pidos, eficientes y accesibles**.

En este artÃ­culo te cuento **quÃ© trae de nuevo DeepSeek**, **cÃ³mo estas innovaciones estÃ¡n marcando tendencia** y **por quÃ© van a cambiar la forma en que las empresasâ€”grandes y pequeÃ±asâ€”usan la IA**.  
Vamos al lÃ­o.


## **Â¿QuÃ© es DeepSeek y por quÃ© deberÃ­as prestarle atenciÃ³n?**

DeepSeek es un conjunto de **modelos de inteligencia artificial generativa** desarrollados por *DeepSeek AI*, una startup china que viene pisando fuerte. Han lanzado varios modelos de cÃ³digo abierto (*open-source*), diseÃ±ados para tareas de procesamiento de texto e imÃ¡genes, y estÃ¡n **rompiendo el equilibrio de poder** en el mundo de la IA.

Mientras otros gigantes como OpenAI o Anthropic siguen apostando por modelos cerrados, DeepSeek abre la puerta a que **cualquiera pueda experimentar, integrar y mejorar sus sistemas de IA**, incluso sin ser una gran corporaciÃ³n. Recordemos que otras empresas como es el caso de Meta (Facebook) con sus modelos Llama, ya ofrecen modelos opensource cada vez mÃ¡s potentes.

Pero lo que realmente llama la atenciÃ³n no es solo que son de cÃ³digo abierto. **DeepSeek introduce innovaciones que podrÃ­an marcar el futuro de toda la industria**.


## **Las 5 innovaciones clave que trae DeepSeek (y cÃ³mo afectarÃ¡n al sector)**

AquÃ­ va la lista con *chicha*:


### **1. Arquitectura Mixture of Experts (MoE): solo trabajan los que saben**

**Â¿QuÃ© significa?**  
DeepSeek implementa la **arquitectura Mixture of Experts (MoE)**, un sistema donde no todos los parÃ¡metros del modelo trabajan a la vez. Solo se activan los â€œexpertosâ€ mÃ¡s adecuados para cada tarea. Es como tener un equipo de especialistas donde solo los que realmente saben de un tema se levantan a trabajar.

#### **Â¿Por quÃ© es revolucionario?**
- En lugar de activar los **236.000 millones de parÃ¡metros** de su modelo mÃ¡s grande, **DeepSeek MoE solo usa 39.000 millones de parÃ¡metros por inferencia**.
- Esto reduce de forma drÃ¡stica el **consumo energÃ©tico**.  
  Se estima una **reducciÃ³n de hasta el 80% en consumo de recursos durante la inferencia**, comparado con modelos *dense* tradicionales.
- Los MoE son nuevos por ejemplo la empresa francesa Mistral ya ofrece este tipo de modelos. Pero debido a su dificultad de implementar no son muchos los casos.

$$
\text{Output} = \sum_{i=1}^{N} G_i(x) \cdot E_i(x)
$$

AquÃ­:  
- $G_i(x)$ es el *gate*, que selecciona quÃ© expertos participan.  
- $E_i(x)$ es la respuesta de cada experto.  
- **Solo 2 expertos activos** de un total de 64 en cada operaciÃ³n.

#### **Ventajas**  
âœ… **Inferencias mÃ¡s rÃ¡pidas**  
âœ… **Menor gasto en computaciÃ³n**  
âœ… **Menor impacto ambiental**

#### **Peligros y retos**  
âš ï¸ **FragmentaciÃ³n del conocimiento**: el modelo depende de que los â€œgatesâ€ seleccionen bien a los expertos.  
âš ï¸ **Mayor complejidad en el entrenamiento**: aunque la inferencia es mÃ¡s barata, entrenar bien un MoE es un desafÃ­o.


### **2. Multimodalidad real: texto, imÃ¡genes y pronto vÃ­deo y audio**

**Â¿QuÃ© significa?**  
DeepSeek es un modelo **multimodal**, capaz de procesar y razonar sobre **texto e imÃ¡genes al mismo tiempo**. EstÃ¡n preparando el soporte para **vÃ­deo y audio**, lo que lo pondrÃ­a al nivel de los sistemas *mÃ¡s avanzados* como **Gemini 1.5 y 2.* ** de Google.

#### **Ventajas**  
âœ… Puedes **subir un documento escaneado** (imagen) y pedirle que **extraiga datos y los resuma**.  
âœ… Puedes **mezclar texto e imagen en la misma consulta**, ideal para procesos de soporte al cliente o auditorÃ­as visuales.

#### **Peligros y retos**  
âš ï¸ **Requiere datos multimodales bien etiquetados** para entrenar estos sistemas.  
âš ï¸ **Puede abrir nuevas vÃ­as para el *deepfake* y la manipulaciÃ³n multimedia.**


### **3. Open-source con licencias comerciales generosas**

DeepSeek sigue el modelo de **cÃ³digo abierto**, algo que solo unos pocos estÃ¡n haciendo (como Mistral en Europa y Llama en USA).  
Esto permite **acceso libre y gratuito** a los pesos del modelo y uso comercial **sin necesidad de pagar licencias**, al menos en los tÃ©rminos actuales.

#### **Ventajas**  
âœ… Puedes **integrarlo en tus sistemas internos sin coste**.  
âœ… Accesible para **startups y PYMES** que antes no podÃ­an permitirse IAs de alto nivel.

#### **Peligros y retos**  
âš ï¸ **Dependencia de la comunidad**: si nadie contribuye, el desarrollo podrÃ­a estancarse.  
âš ï¸ **Riesgos de seguridad**: el open-source puede facilitar el acceso a malas prÃ¡cticas si no se controla su uso.


### **4. Eficiencia energÃ©tica en entrenamiento e inferencias**

Uno de los puntos fuertes de DeepSeek es que **reduce el consumo de energÃ­a** tanto en el entrenamiento como en la inferencia gracias a su arquitectura MoE.

#### **Datos clave**  
- Un modelo denso de 175B como GPT-3 consume **1287 MWh** durante el entrenamiento (estos datos los he extraÃ­do de las Ãºltimas estimaciones).  
- Un MoE bien implementado puede **reducir este consumo en un 50-60%** en el entrenamiento y hasta **un 80% en la inferencia**.

#### **Ventajas**  
âœ… **Menor huella de carbono**.  
âœ… **Abaratamiento de la IA**: menos energÃ­a significa **menos coste operativo**.

#### **Peligros y retos**  
âš ï¸ **Los ahorros dependen del uso eficiente del MoE**.  
âš ï¸ **Las infraestructuras aÃºn deben adaptarse a estos nuevos modelos**.


### **5. Entrenamiento multilingÃ¼e**

DeepSeek se ha entrenado desde cero con datos **multilingÃ¼es**, incluyendo un **buen corpus en castellano**.  
Esto lo coloca por delante de otros modelos que **traducen al vuelo** y terminan dando respuestas menos naturales.

#### **Ventajas**  
âœ… Mejora la **comprensiÃ³n y generaciÃ³n de contenido en castellano**.  
âœ… Ideal para **PYMES que venden en mercados donde no se habla inglÃ©s**.

#### **Peligros y retos**  
âš ï¸ **Calidad variable segÃºn el idioma**: aunque ha mejorado, el 100% de consistencia aÃºn es difÃ­cil de lograr.


## **Â¿QuÃ© significa esto para el resto del sector?**

Dado que los datos de entrenamiento e inferencia se han hecho pÃºblico esto va a impulsar a todo el sector, que podrÃ¡ implementar estas mejoras de manera inmediata. AdemÃ¡s el Ã©xito de DeepSeek en implementar estas tecnologÃ­as **obliga al resto de los gigantes a reaccionar**. AquÃ­ te cuento lo que ya estÃ¡ pasando (o va a pasar sÃ­ o sÃ­):

| **Empresa**   | **QuÃ© harÃ¡n**                                                                 |
|---------------|-------------------------------------------------------------------------------|
| **OpenAI**    | EstÃ¡n explorando **MoE** para los prÃ³ximos modelos GPT. La eficiencia energÃ©tica es clave para su sostenibilidad. |
| **Google (Gemini)** | Gemini 1.5 2.* ya son multimodales y estÃ¡ usando tÃ©cnicas similares de *expert gating* para reducir costes. |
| **Anthropic** | Claude 3 incorporarÃ¡ **arquitecturas hÃ­bridas**, y se espera que adopten estrategias MoE en su entrenamiento. |
| **Meta**      | Meta AI estÃ¡ apostando por **modelos open-source** y tambiÃ©n por **MoE**, como en LLaMA 3. |

> La competencia **va a democratizar** aÃºn mÃ¡s la IA, pero tambiÃ©n generarÃ¡ una **carrera por la eficiencia** que beneficiarÃ¡ a los usuariosâ€¦ y presionarÃ¡ los precios.


## **Ventajas y peligros de este nuevo panorama**

### **Ventajas**
- **IA mÃ¡s accesible** para empresas pequeÃ±as.  
- **Costes operativos menores** por la reducciÃ³n en consumo energÃ©tico.  
- **Mayor velocidad de respuesta y flexibilidad**.

### **Peligros**
- **Sobreoferta y confusiÃ³n**: muchas opciones pueden hacer difÃ­cil elegir.  
- **Riesgos de seguridad y privacidad** si se usan modelos open-source sin los controles adecuados.  
- **Dependencia tecnolÃ³gica**: cuanto mÃ¡s fÃ¡cil es usar IA, mÃ¡s dependientes podemos volvernos de ella.


Â¡Buena observaciÃ³n! AquÃ­ te dejo cÃ³mo podrÃ­amos explicar esa **ventaja/peligro** que comentas, y aclarar la situaciÃ³n para que quede bien entendido en el artÃ­culo.


## **Â¿Menos consumo significa menos inversiones? No exactamente.**

Una de las **dudas** que estÃ¡ surgiendo en el sector es si estas nuevas arquitecturas eficientes, como la **Mixture of Experts (MoE)** de DeepSeek, harÃ¡n que **las grandes empresas dejen de invertir en infraestructura de IA**. Y si eso ocurre, Â¿quÃ© pasa con empresas como **NVIDIA**, que vive de vender sus GPUs a centros de datos de IA?

### **Este es el razonamiento que inquieta a algunos inversores sin conocimientos de IA:**
> *Si los modelos consumen menos energÃ­a y requieren menos potencia de cÃ³mputo para entrenarse o realizar inferencias, quizÃ¡ las empresas necesiten **comprar menos hardware** o gastarse menos dinero en computaciÃ³n.*

Esto **podrÃ­a parecer un riesgo** para el crecimiento de empresas tecnolÃ³gicas que dependen de la **demanda masiva de GPUs** y chips especÃ­ficos para IA.


### **Pero la realidad es otra: estas inversiones serÃ¡n mÃ¡s eficientes, no menores.**

Aunque los modelos **sean mÃ¡s eficientes**, la **demanda de IA** estÃ¡ creciendo **exponencialmente**. Las **mejoras en eficiencia**, como las que ha introducido DeepSeek, lo que permiten es **hacer mÃ¡s con el mismo presupuesto**, no hacer menos.

#### **Piensa en esto asÃ­:**
> Antes, una empresa tenÃ­a que invertir una **barbaridad** para entrenar un modelo decente. Ahora, puede **invertir lo mismo o incluso mÃ¡s**, pero obteniendo **modelos mucho mÃ¡s potentes y complejos**.

âœ… **MÃ¡s eficiencia no frena el gasto, lo multiplica hacia nuevas aplicaciones.**  
âœ… Empresas que antes no podÃ­an entrar en el juego de la IA ahora sÃ­ pueden, **aumentando el mercado total de IA**.

#### **NVIDIA y el resto de tecnolÃ³gicas se benefician porque:**
1. La **demanda global de IA crece**: mÃ¡s empresas, mÃ¡s paÃ­ses, mÃ¡s sectores.
2. **El uso de GPUs se diversifica**, no solo en entrenamientos bestiales, sino en **miles de modelos personalizados y aplicaciones concretas**.
3. **La inversiÃ³n no se reduce, se redistribuye** en soluciones **mÃ¡s escalables y extendidas**.


### **La Paradoja de Jevons: cuando mÃ¡s eficiencia genera mÃ¡s consumo**

Esta paradoja fue planteada por el economista britÃ¡nico **William Stanley Jevons** en el siglo XIX. Ã‰l observÃ³ que a medida que las mÃ¡quinas de vapor se volvÃ­an **mÃ¡s eficientes** en el consumo de carbÃ³n, **no disminuyÃ³ el uso de carbÃ³n**. Al contrario: **el consumo total aumentÃ³**, porque la eficiencia hacÃ­a que **mÃ¡s industrias y personas usaran esas mÃ¡quinas**, y la **demanda general** de energÃ­a se disparÃ³.

#### **En el contexto de la IA, pasa algo parecido:**
> A medida que los modelos de inteligencia artificial se vuelven **mÃ¡s eficientes** (gracias a arquitecturas como **Mixture of Experts**), **no es que disminuya el uso de recursos**; lo que ocurre es que **muchas mÃ¡s empresas empiezan a usar IA**, se entrenan **mÃ¡s modelos** y se crean **mÃ¡s aplicaciones**.

Este fenÃ³meno explica por quÃ©, aunque DeepSeek y otros modelos consumen **menos energÃ­a por operaciÃ³n**, **la demanda total de cÃ³mputo e inversiÃ³n en IA seguirÃ¡ creciendo**.


### **CÃ³mo afecta la Paradoja de Jevons a la IA (y a tu negocio)**

âœ… Las **mejoras en eficiencia** harÃ¡n que la IA estÃ© al alcance de **mÃ¡s empresas**, incluso PYMES que antes ni lo consideraban.  
âœ… **MÃ¡s uso** de IA significa **mÃ¡s datos, mÃ¡s servicios inteligentes** y **mÃ¡s competencia en todos los sectores**.  
âœ… Para los grandes fabricantes como **NVIDIA**, lejos de significar un frenazo, es una **oportunidad para vender mÃ¡s GPUs**, pero **a un mercado mucho mÃ¡s amplio**.


> **La eficiencia no frena el progreso, lo acelera.**  
> Con DeepSeek y su arquitectura eficiente, lo que veremos es una **explosiÃ³n de adopciÃ³n** de IA. Esto generarÃ¡ **mÃ¡s consumo global de recursos**, aunque **de manera mÃ¡s inteligente y optimizada**.


## **DeepSeek no es una moda, es un adelanto del futuro**

DeepSeek marca un antes y un despuÃ©s en la carrera de la inteligencia artificial. Sus avances en eficiencia energÃ©tica, multimodalidad y accesibilidad **ya estÃ¡n inspirando a los grandes del sector** a cambiar de rumbo.

> Si tienes un negocio y quieres aprovechar la IA sin complicaciones ni grandes inversiones, **este es el momento**.

---

## **Â¿Hablamos de cÃ³mo aplicar DeepSeek en tu empresa?**

Yo ya estoy probando estas tecnologÃ­as en proyectos reales.  
ğŸ‘‰ EscrÃ­beme en [www.vialabsdigital.com](http://www.vialabsdigital.com) y vemos cÃ³mo adaptarlas a tu negocio.


