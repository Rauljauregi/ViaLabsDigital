---

title: "Qu√© son los Transformers en la IA"
description: "Explora c√≥mo funcionan los transformers en el procesamiento de lenguaje natural con una gu√≠a clara y paso a paso. Entiende conceptos clave como self-attention y su aplicaci√≥n en clasificaci√≥n de texto."
category: Deep Learning
pubDate: "2025-01-21T20:00:00.000Z"
heroImage: '/images/Qu√© son los Transformers en la IA.webp'
tags:
  - Transformers
  - Bert
  - Caso pr√°ctico
---

Si alguna vez te has preguntado c√≥mo los modelos de inteligencia artificial logran entender el lenguaje humano con una precisi√≥n casi m√°gica, los transformers son los responsables de esta revoluci√≥n. Hoy vas a entender los Transformers en la IA.&#x20;

En el fascinante mundo del procesamiento del lenguaje natural (NLP), los transformers han cambiado las reglas del juego. Hoy te guiar√© paso a paso para entenderlos.

En este art√≠culo, exploraremos un transformer que utiliza **√∫nicamente un encoder** para la clasificaci√≥n de texto. Adem√°s, te presentar√© un **notebook interactivo** donde podr√°s experimentar c√≥mo este modelo procesa y transforma palabras en predicciones.

## ¬øQu√© es un Transformer que solo tiene Encoder?

Un transformer con solo encoder analiza una secuencia de texto y genera representaciones internas que capturan las relaciones entre palabras. Esto es √∫til para tareas como **clasificaci√≥n de texto**, **extracci√≥n de informaci√≥n** o **an√°lisis de sentimientos**.

El coraz√≥n del modelo es el mecanismo de **self-attention**, que eval√∫a cu√°nto influye cada palabra en el resto de la secuencia. Matem√°ticamente, se expresa as√≠:

$$
Q = XW_Q, \; K = XW_K, \; V = XW_V
$$

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Donde \$Q\$, \$K\$ y \$V\$ son matrices que representan palabras transformadas mediante pesos aprendidos (\$W\_Q\$, \$W\_K\$, \$W\_V\$). Este mecanismo permite al modelo determinar la importancia relativa de cada palabra.

## ¬øPor qu√© usar un Transformer para Clasificaci√≥n de Texto?

Los transformers entienden el **contexto** de una oraci√≥n completa, a diferencia de los modelos anteriores que solo analizaban palabras individuales. Esto los hace ideales para:

- **An√°lisis de sentimientos**: Determinar si una rese√±a es positiva o negativa.
- **Detecci√≥n de spam**: Filtrar correos no deseados.
- **Clasificaci√≥n de tickets de soporte**: Priorizar solicitudes seg√∫n su urgencia.

## El Notebook: Un Transformer Paso a Paso

Para entender mejor este proceso, hemos creado un **notebook en Google Colab**. Puedes explorarlo aqu√≠:

<a href="https://colab.research.google.com/drive/1zUpluuIwXAoKM0T6AFcQN9jYqvh4vTJc" target="_blank" rel="noopener">üëâ Abrir el notebook en Colab aqu√≠</a>

A continuaci√≥n, resumimos los pasos principales del modelo:

### 1. Creaci√≥n del Dataset para Clasificaci√≥n de Texto

El dataset consiste en frases simples etiquetadas como positivas (1) o negativas (0):

```python
# Ejemplo de dataset:
data = [
    ("I love this movie", 1),
    ("This film was terrible", 0)
]
```

**‚ö†Ô∏è Nota importante:**
Hemos utilizado **todo el dataset para entrenamiento**, lo cual **NO es correcto en un modelo real**.
Esto hace que el modelo **memorice los datos** en lugar de aprender patrones generales. En un caso real, siempre debemos separar un conjunto de prueba.

### 2. Creaci√≥n del Vocabulario y Representaci√≥n del Texto

Convertimos palabras en √≠ndices num√©ricos para que el modelo pueda procesarlas.

```python
vocab = {"love": 0, "movie": 1, "film": 2, "was": 3, "terrible": 4}
```

As√≠, una frase como `"I love this movie"` se convierte en:

$$
\text{Embedded vectors: } E = XW_E
$$

### 3. Definici√≥n del Modelo Transformer

El modelo incluye:

- **Embeddings** para convertir palabras en vectores.
- **Capa de self-attention** para analizar relaciones entre palabras.
- **Clasificador final** que predice si el texto es positivo o negativo.

### 4. Entrenamiento del Modelo

Entrenamos el modelo durante varias √©pocas y monitoreamos la p√©rdida para verificar su aprendizaje.

```python
√âpoca 1/10, P√©rdida: 0.7291
√âpoca 10/10, P√©rdida: 0.5199
```

### 5. Evaluaci√≥n y Precisi√≥n del Modelo

El modelo alcanza **75% de precisi√≥n** en el conjunto de prueba:

```python
Precisi√≥n en el conjunto de prueba: 75.00%
```

### 6. Visualizaci√≥n de Transformaciones del Modelo

#### **Embeddings**

$$
E = XW_E
$$



#### **Matrices de Atenci√≥n y Pesos de Atenci√≥n**

$$
\text{Attention weights: } \text{Softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)
$$



### 7. Inferencia y Errores del Modelo

Al probar con nuevas frases, observamos fallos en la predicci√≥n:

```python
Frase: "Absolutely love this film"
Predicci√≥n: Negativo
Probabilidades: [0.6649, 0.3351]
```

**¬øPor qu√© fall√≥?**
1Ô∏è‚É£ La palabra **"Absolutely"** no estaba en el dataset de entrenamiento.
2Ô∏è‚É£ El modelo puede no haber aprendido la relaci√≥n entre **"love"** y **"film"**.
3Ô∏è‚É£ El dataset es peque√±o, por lo que el modelo **no generaliza bien**.

### **¬øC√≥mo Mejorar el Modelo?**

‚úÖ Ampliar el dataset con m√°s ejemplos.
‚úÖ Usar embeddings preentrenados (ej. Word2Vec, BERT).
‚úÖ Aumentar `num_heads` en la autoatenci√≥n.
‚úÖ Entrenar por m√°s √©pocas (`epochs=50`).

---


Espero que este art√≠culo y el notebook interactivo te hayan ayudado a entender mejor c√≥mo funcionan los transformers paso a paso. Ahora es tu turno: experimenta con el c√≥digo que he preparado:<a href="https://colab.research.google.com/drive/1zUpluuIwXAoKM0T6AFcQN9jYqvh4vTJc" target="_blank" rel="noopener">üëâ Abrir el notebook aqu√≠</a> y descubre nuevas aplicaciones para tus proyectos. ¬°Estoy seguro de que te sorprender√°s con todo lo que puedes lograr!

¬°Nos leemos pronto y que la curiosidad por aprender nunca falte!

