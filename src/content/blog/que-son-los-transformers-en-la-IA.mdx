---

title: "Qu√© son los Transformers en la IA"
description: "Explora c√≥mo funcionan los transformers en el procesamiento de lenguaje natural con una gu√≠a clara y paso a paso. Entiende conceptos clave como self-attention y su aplicaci√≥n en clasificaci√≥n de texto."
category: Deep Learning
pubDate: "2025-01-21T20:00:00.000Z"
heroImage: '/images/Qu√© son los Transformers en la IA.webp'
tags:
  - Transformers
  - Bert
  - Caso pr√°ctico
---

Si alguna vez te has preguntado c√≥mo los modelos de inteligencia artificial logran entender el lenguaje humano con una precisi√≥n casi m√°gica, los transformers son los responsables de esta revoluci√≥n. Hoy vas a entender los Transformers en la IA.&#x20;

En el fascinante mundo del procesamiento del lenguaje natural (NLP), los transformers han cambiado las reglas del juego. Hoy te guiar√© paso a paso para entenderlos con un ejemplo muy sencillo.

En este art√≠culo, he creado un transformer que utiliza **√∫nicamente un encoder** para la clasificaci√≥n de texto. Adem√°s, puedes interactuar con √©l en un **notebook interactivo** donde podr√°s experimentar c√≥mo este modelo procesa y transforma palabras en predicciones.

## ¬øQu√© es un Transformer que solo tiene Encoder?

Un transformer con solo encoder analiza una secuencia de texto y genera representaciones internas que capturan las relaciones entre palabras. Esto es √∫til para tareas como **clasificaci√≥n de texto**, **extracci√≥n de informaci√≥n** o **an√°lisis de sentimientos**.


### Los Transformers

El modelo **Transformer**, presentado en el art√≠culo <a href="https://es.wikipedia.org/wiki/Transformador_(modelo_de_aprendizaje_autom%C3%A1tico)" target="_blank" rel="noopener">Attention Is All You Need</a>, es la arquitectura base de modelos avanzados de procesamiento del lenguaje como GPT y BERT. En su dise√±o original, el Transformer consta de dos componentes principales: **el encoder y el decoder**.

Sin embargo, m√°s que una √∫nica arquitectura, los Transformers pueden entenderse como un conjunto de herramientas modulares. Para este art√≠culo, nos centraremos √∫nicamente en la implementaci√≥n del **encoder**, dejando fuera tanto el **positional encoding** (que explicaremos en otro art√≠culo) como el **decoder**, que utilizaremos m√°s adelante para construir una versi√≥n simplificada de un modelo GPT.

#### Esquema del Transformer
A continuaci√≥n, presentamos el esquema original del Transformer:

<img 
  src="/images/transformer-schema.png" 
  alt="Esquema de un Transformer completo" 
  title="Esquema de un Transformer completo" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>

> En nuestro ejemplo adem√°s de 'The Encoder' vamos a implementar el bloque indicado como 'Input Embedding' pero no el 'Positional Encoding'.

> Adem√°s no vamos a tokenizar las palabras para simplificar al m√°ximo todo el proceso. <a href="https://mindfulml.vialabsdigital.com/post/tokenizacin-para-modelos-de-lenguaje/" target="_blank" rel="noopener">Para saber m√°s sobre tokenizadores pulsa aqu√≠</a>.


El coraz√≥n del modelo es el mecanismo de **self-attention**, que eval√∫a cu√°nto influye cada palabra en el resto de la secuencia. Matem√°ticamente, se expresa as√≠:

$$
Q = XW_Q, \; K = XW_K, \; V = XW_V
$$

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Donde $Q$, $K$ y $V$ son matrices que representan palabras transformadas mediante pesos aprendidos ($W_Q$, $W_K$, $W_V$). Este mecanismo permite al modelo determinar la importancia relativa de cada palabra.

## ¬øPor qu√© usar un Transformer para Clasificaci√≥n de Texto?

Los transformers entienden el **contexto** de una oraci√≥n completa, a diferencia de los modelos anteriores que solo analizaban palabras individuales. Esto los hace ideales para:

- **An√°lisis de sentimientos**: Determinar si una rese√±a es positiva o negativa.
- **Detecci√≥n de spam**: Filtrar correos no deseados.
- **Clasificaci√≥n de tickets de soporte**: Priorizar solicitudes seg√∫n su urgencia.

## El Notebook: Un Transformer Paso a Paso

Para entender mejor este proceso, he creado un **notebook en Google Colab**. Puedes verlo aqu√≠:

<a href="https://colab.research.google.com/drive/1zUpluuIwXAoKM0T6AFcQN9jYqvh4vTJc" target="_blank" rel="noopener">üëâ Abrir el notebook en Colab aqu√≠</a>

A continuaci√≥n, est√°n resumidos los pasos principales del modelo:

### 1. Creaci√≥n del Dataset para Clasificaci√≥n de Texto

El dataset consiste en frases simples etiquetadas como positivas (1) o negativas (0):

```python
# Ejemplo de dataset:
data = [
    ("I love this movie", 1),
    ("This film was terrible", 0),
    ("An excellent performance", 1),
    ("Worst movie ever", 0),
    ("Fantastic storyline", 1),
    ("Not worth watching", 0)
]
```

**‚ö†Ô∏è Nota importante:**
He utilizado **todo el dataset para entrenamiento**, lo cual **NO es correcto en un modelo real**.
Esto hace que el modelo **memorice los datos** en lugar de aprender patrones generales. En un caso real, siempre debemos separar un conjunto de prueba.

### 2. Creaci√≥n del Vocabulario y Representaci√≥n del Texto

Convertimos palabras en √≠ndices num√©ricos para que el modelo pueda procesarlas.

```python
vocabulario: 
 {'An': 0, 'Fantastic': 1, 'I': 2, 'Not': 3,
  'This': 4, 'Worst': 5, 'ever': 6, 'excellent': 7,
  'film': 8, 'love': 9, 'movie': 10, 'performance': 11, 
  'storyline': 12, 'terrible': 13, 'this': 14, 'was': 15, 
  'watching': 16, 'worth': 17, '<UNK>': 18}
```

As√≠, una frase como `"I love this movie"` se convierte en:

```python
[ 2,  9, 14, 10]
```

$$
\text{Embedded vectors: } E = XW_E
$$

<img 
  src="/images/paso2-transformer.png" 
  alt="paso 2 Transformer" 
  title="paso 2 Transformer" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>


### 3. Definici√≥n del Modelo Transformer

El modelo incluye:

- **Embeddings** para convertir palabras en vectores.
- **Capa de self-attention** para analizar relaciones entre palabras.
- **Clasificador final** que predice si el texto es positivo o negativo.


<img 
  src="/images/paso3-transformer.png" 
  alt="paso 3 Transformer" 
  title="paso 3 Transformer" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>

### 4. Entrenamiento del Modelo

Entrenamos el modelo durante varias √©pocas y monitoreamos la p√©rdida para verificar su aprendizaje.

```python
√âpoca 1/10, P√©rdida: 0.7291
...
√âpoca 10/10, P√©rdida: 0.5199
```

<img 
  src="/images/paso4-transformer.png" 
  alt="paso 4 Transformer" 
  title="paso 4 Transformer" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>

### 5. Evaluaci√≥n y Precisi√≥n del Modelo

El modelo alcanza **75% de precisi√≥n** en el conjunto de prueba:

```python
Precisi√≥n en el conjunto de prueba: 75.00%
```

### 6. Visualizaci√≥n de Transformaciones del Modelo

<img 
  src="/images/paso6-1-transformer.png" 
  alt="paso 6-1 Transformer" 
  title="paso 6-1 Transformer" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>

#### **Embeddings**

$$
E = XW_E
$$

<img 
  src="/images/paso6-2-transformer.png" 
  alt="paso 6-2 Transformer" 
  title="paso 6-2 Transformer" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>


#### **Matrices de Atenci√≥n y Pesos de Atenci√≥n**

<img 
  src="/images/paso6-3-transformer.png" 
  alt="paso 6-3 Transformer" 
  title="paso 6-3 Transformer" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>

$$
\text{Attention weights: } \text{Softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)
$$

<img 
  src="/images/paso6-4-transformer.png" 
  alt="paso 6-4 Transformer" 
  title="paso 6-4 Transformer" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>

<img 
  src="/images/paso6-5-transformer.png" 
  alt="paso 6-5 Transformer" 
  title="paso 6-5 Transformer" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>

<img 
  src="/images/paso6-6-transformer.png" 
  alt="paso 6-6 Transformer" 
  title="paso 6-6 Transformer" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>

### 7. Inferencia y Errores del Modelo

<img 
  src="/images/paso7-transformer.png" 
  alt="paso 7 Transformer" 
  title="paso 7 Transformer" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>

Al probar con nuevas frases, observamos un fallo en la predicci√≥n:

```python
Frase: "Absolutely love this film"
Predicci√≥n: Negativo
Probabilidades: [0.6649, 0.3351]
```

**¬øPor qu√© fall√≥?**
1Ô∏è‚É£ La palabra **"Absolutely"** no estaba en el dataset de entrenamiento.  
2Ô∏è‚É£ El modelo puede no haber aprendido la relaci√≥n entre **"love"** y **"film"**.  
3Ô∏è‚É£ El dataset es peque√±o, por lo que el modelo **no generaliza bien**.  

### **¬øC√≥mo Mejorar el Modelo?**

‚úÖ Ampliar el dataset con m√°s ejemplos.  
‚úÖ Usar embeddings preentrenados (ej. Word2Vec, BERT).  
‚úÖ Aumentar `num_heads` en la autoatenci√≥n.  
‚úÖ Entrenar por m√°s √©pocas (`epochs=50`).  

Pero la propia simplicidad del modelo que hemos hecho permite entender muy bien c√≥mo funciona un transformer.

---


Espero que este art√≠culo y el notebook interactivo te hayan ayudado a entender mejor c√≥mo funcionan los transformers paso a paso. Ahora es tu turno: experimenta con el c√≥digo que he preparado:<a href="https://colab.research.google.com/drive/1zUpluuIwXAoKM0T6AFcQN9jYqvh4vTJc" target="_blank" rel="noopener">üëâ Abrir el notebook aqu√≠</a> y descubre nuevas aplicaciones para tus proyectos. ¬°Estoy seguro de que te sorprender√°s con todo lo que puedes lograr!

¬°Nos leemos pronto y que la curiosidad por aprender nunca falte!


Ra√∫l J√°uregui  
Consultor IA e I+D+i  
<a href="https://vialabsdigital.com" target="_blank" rel="noopener">ViaLabs digital</a>  
Si tienes un proyecto puedo ayudarte a hacerlo realidad.  
