---
title: "Qu茅 son los Transformers en la IA"
description: >-
    "Explora c贸mo funcionan los transformers en el procesamiento de lenguaje natural con una gu铆a clara y paso a paso. Entiende conceptos clave como self-attention y su aplicaci贸n en clasificaci贸n de texto."
category: Deep Learning
pubDate: "2025-01-21T20:00:00.000Z"
heroImage: '/images/Qu茅 son los Transformers en la IA.webp'
tags:
  - Transformers
  - Bert
  - Caso pr谩ctico
---

Si alguna vez te has preguntado c贸mo los modelos de inteligencia artificial logran entender el lenguaje humano con una precisi贸n casi m谩gica, d茅jame decirte que los transformers son los magos detr谩s de todo esto. En el fascinante mundo del procesamiento del lenguaje natural (NLP), los transformers han cambiado las reglas del juego, y hoy quiero compartir contigo una gu铆a sencilla y visual para entenderlos.

En este art铆culo, te llevar茅 paso a paso a trav茅s de un transformer que utiliza 煤nicamente un encoder. Adem谩s, te presentar茅 un notebook interactivo donde podr谩s experimentar de primera mano c贸mo este modelo transforma palabras de entrada en salidas 煤tiles. 隆Prep谩rate para desentra帽ar el misterio!

## 驴Qu茅 es un Transformer que solo tiene Encoder?

Imagina que tienes un texto y necesitas entender cada palabra en funci贸n de las dem谩s. Un transformer con solo encoder hace justamente eso: analiza la secuencia de texto y genera representaciones que capturan las relaciones internas entre palabras. Este enfoque es perfecto para tareas como la clasificaci贸n de texto, la extracci贸n de caracter铆sticas o incluso el an谩lisis de sentimientos.

El alma de este modelo es el mecanismo de **self-attention**, que act煤a como un detective que analiza c贸mo cada palabra se conecta con las dem谩s en la secuencia. Matem谩ticamente, esto se ve as铆:

$$
Q = XW_Q, \; K = XW_K, \; V = XW_V
$$

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Aqu铆, $Q$, $K$ y $V$ son matrices que representan las palabras de entrada transformadas mediante pesos aprendidos ($W_Q$, $W_K$, $W_V$). Esto le permite al modelo asignar importancia relativa a cada palabra.

## 驴Por qu茅 utilizar un Transformer para Clasificaci贸n de Texto?

Los transformers son como esos amigos que entienden todo el contexto de una conversaci贸n, no solo las palabras sueltas. Son ideales para tareas de clasificaci贸n porque crean representaciones contextuales de las palabras, entendiendo cada una en funci贸n de las dem谩s.

### Algunos ejemplos de su uso en la vida real:

- **An谩lisis de sentimientos:** Determinar si una rese帽a es positiva o negativa.
- **Detecci贸n de spam:** Identificar esos correos que no queremos ver.
- **Clasificaci贸n de tickets de soporte:** Ordenar tickets seg煤n su urgencia.

## El Notebook: Un Transformer Paso a Paso

Para que no todo quede en teor铆a, he creado un <a href="https://colab.research.google.com/drive/1zUpluuIwXAoKM0T6AFcQN9jYqvh4vTJc" target="_blank" rel="noopener"> notebook interactivo en Google Colab</a>.

Aqu铆 te detallo los pasos clave para que lo explores a fondo:

### 1. Introducci贸n y Dataset para Clasificaci贸n de Texto

Creamos un dataset simple con frases etiquetadas como positivas o negativas. Algo peque帽o pero efectivo para entender c贸mo trabaja el modelo.

```python
# Ejemplo de dataset:
data = [
    ("I love this movie", 1),
    ("This film was terrible", 0)
]
```

### 2. Creaci贸n del Vocabulario y Representaci贸n de Texto como ndices

Convertimos las palabras en 铆ndices num茅ricos usando un vocabulario sencillo. Esto permite que el modelo procese el texto de manera eficiente.

$$
\text{Embedded vectors: } E = XW_E
$$

### 3. Definici贸n del Modelo BERT Simplificado

Creamos un transformer simplificado con PyTorch, que incluye:
- Embeddings para convertir palabras en vectores.
- Una capa de self-attention para analizar relaciones entre palabras.
- Una capa de clasificaci贸n para predecir si el texto es positivo o negativo.

### 4. Entrenamiento del Modelo en un Dataset Peque帽o

Entrenamos el modelo durante varias 茅pocas y monitoreamos la p茅rdida para asegurarnos de que aprende correctamente.

### 5. Visualizaci贸n de Transformaciones Internas del Modelo

#### Embeddings
As铆 se ven las palabras transformadas en vectores:

$$
E = XW_E
$$


<img 
  src="/images/visualizacion-embeddings.png" 
  alt="Visualizaci贸n de Embeddings" 
  title="Visualizaci贸n de Embeddings" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>

#### Matrices Q, K, V y Pesos de Atenci贸n

$$
Q = EW_Q, \; K = EW_K, \; V = EW_V
$$

$$
\text{Attention weights: } \text{Softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)
$$



<img 
  src="/images/pesos-atencion.png" 
  alt="Pesos de Atenci贸n" 
  title="Pesos de Atenci贸n" 
  style="width: 100%; height: auto; margin: 20px 0; border-radius: 8px;"
/>


### 6. Inferencia con Frases Nuevas

Por 煤ltimo, usamos el modelo para predecir si nuevas frases son positivas o negativas. Por ejemplo:

```python
new_text = "Fantastic movie ever"
prediction = model.predict(new_text)
```

El modelo clasificar谩 la frase y mostrar谩 las probabilidades asociadas a cada clase.

## Visualizaci贸n: C贸mo Entender las Transformaciones del Transformer

- **Embeddings:** Representan cada palabra como un vector en un espacio multidimensional.
- **Pesos de atenci贸n:** Capturan c贸mo cada palabra influye en las dem谩s.
- **Proyecci贸n final:** Resume la informaci贸n en un vector utilizado para la clasificaci贸n.



Espero que este art铆culo y el notebook interactivo te hayan ayudado a entender mejor c贸mo funcionan los transformers paso a paso. Ahora es tu turno: experimenta con el c贸digo que he preparado:<a href="https://colab.research.google.com/drive/1zUpluuIwXAoKM0T6AFcQN9jYqvh4vTJc" target="_blank" rel="noopener"> Abrir el notebook aqu铆</a> y descubre nuevas aplicaciones para tus proyectos. 隆Estoy seguro de que te sorprender谩s con todo lo que puedes lograr!

隆Nos leemos pronto y que la curiosidad por aprender nunca falte!

