---
title: "CÃ³mo aprende una red neuronal (y cÃ³mo automatizar tareas en tu empresa)"
description: >-
  CÃ³mo funcionan las redes neuronales artificiales, el descenso de gradiente y la retropropagaciÃ³n. Aprende cÃ³mo entrenar una IA paso a paso y cÃ³mo aplicarla en la automatizaciÃ³n de tareas en tu negocio.
category: Machine Learning
pubDate: 2025-03-14
heroImage: "/images/como-aprende-red-neuronal.jpg"
tags:
  - Conceptos
  - Redes neuronales
  - Pyme
---

## **CÃ³mo aprende una red neuronal (y cÃ³mo automatizar tareas en tu empresa)**

Â¿Sabes **quÃ© pasa realmente dentro de una red neuronal cuando "aprende"?** Hoy te lo explico con ejemplos reales, un *notebook* prÃ¡ctico, y la teorÃ­a matemÃ¡tica que hace que la magia ocurra.

Vamos a entrenar una **red neuronal sencilla** para clasificar correos de clientes en *quejas*, *preguntas* u *otros*. Pero no es solo cÃ³digo: quiero que entiendas el proceso de aprendizaje paso a paso, incluyendo cÃ³mo funciona el **descenso de gradiente** y la **retropropagaciÃ³n**.

---

## 1. **Â¿QuÃ© es una red neuronal y cÃ³mo aprende?**

Una red neuronal es como un **equipo de empleados** que colaboran para resolver problemas. Cada *neurona* recibe datos, hace sus cÃ¡lculos y pasa el resultado a la siguiente.

**Â¿CÃ³mo aprende?**  
Ajustando los **pesos** de cada conexiÃ³n. Es como dar *feedback* a tus empleados tras cada tarea, para que la prÃ³xima vez lo hagan mejor. En el mundo de la IA, ese proceso se llama **descenso de gradiente** y **retropropagaciÃ³n**.

---

## 2. **El proceso de aprendizaje: teorÃ­a y prÃ¡ctica**

> **Piensa en un empleado nuevo.** Le das tareas, se equivoca, le das feedback y aprende.  
> En una red neuronal ocurre lo mismo. Solo que el *feedback* es una operaciÃ³n matemÃ¡tica llamada **retropropagaciÃ³n**, y los ajustes se realizan mediante el **descenso de gradiente**.

---

## 3. **Las fÃ³rmulas detrÃ¡s del aprendizaje**

### 3.1. **Forward Pass: el camino hacia la predicciÃ³n**

Cada neurona realiza dos pasos:

1ï¸âƒ£ Calcula una **combinaciÃ³n lineal** de las entradas:  
`z = w * x + b`  
- **w**: pesos  
- **x**: entrada  
- **b**: sesgo (bias)

2ï¸âƒ£ Aplica una **funciÃ³n de activaciÃ³n**, por ejemplo, ReLU:  
`a = max(0, z)`  
Esto decide si pasa la informaciÃ³n o no.

---

### 3.2. **FunciÃ³n de pÃ©rdida: medir el error**

DespuÃ©s de la predicciÃ³n, necesitamos saber **quÃ© tan lejos estamos de la respuesta correcta**.

En clasificaciÃ³n multiclase usamos **entropÃ­a cruzada**:  
`L = - Î£ [ yáµ¢ Â· log(Å·áµ¢) ]`  
- **yáµ¢**: valor real (0 o 1 segÃºn la clase correcta)  
- **Å·áµ¢**: probabilidad que predijo el modelo para la clase *i*  
- **C**: nÃºmero de clases (en nuestro ejemplo, **3**)

---

### 3.3. **RetropropagaciÃ³n: el feedback**

Una vez calculado el error, lo enviamos hacia atrÃ¡s para ajustar los pesos.

La derivada de la pÃ©rdida respecto a cada peso (`âˆ‚L / âˆ‚w`) nos dice en quÃ© direcciÃ³n hay que moverlo.

---

### 3.4. **Descenso de gradiente: actualizar los pesos**

Ahora actualizamos los pesos usando el gradiente:  
`w = w - Î· * (âˆ‚L / âˆ‚w)`  
- **Î·**: tasa de aprendizaje (*learning rate*)  
- **âˆ‚L / âˆ‚w**: gradiente (la pendiente que indica cuÃ¡nto ajustar)

> **Imagina bajar una montaÃ±a buscando el punto mÃ¡s bajo.**  
> El gradiente te dice hacia dÃ³nde bajar, y la tasa de aprendizaje es el tamaÃ±o del paso.

---

## 4. **El notebook paso a paso (cÃ³digo + teorÃ­a)**

Ahora que sabes cÃ³mo aprende una red neuronal, te invito a verlo en acciÃ³n.  
ğŸ‘‰ **Abre el notebook en Colab aquÃ­:**  
[ğŸ”— Ir al notebook](https://colab.research.google.com/drive/1Oa8ok09oy4P6YFNlHvQzXuUma9mKD6x6?usp=sharing)

---

### ğŸ“Œ **Paso 1: Preparar el entorno**

Instala las librerÃ­as necesarias:  
```python
!pip install tensorflow nltk scikit-learn pandas matplotlib seaborn
```

---

### ğŸ“Œ **Paso 2: Crear el dataset de correos**

Ejemplo de datos:  
```python
data = {
    "texto": [
        "No estoy satisfecho con el servicio, me cobraron de mÃ¡s.",
        "Â¿CuÃ¡nto cuesta el envÃ­o a Madrid?",
        "Gracias por la atenciÃ³n, todo estuvo excelente."
    ],
    "categoria": ["queja", "pregunta", "otro"]
}

df = pd.DataFrame(data)
```

---

### ğŸ“Œ **Paso 3: Limpiar y preparar los datos**

1ï¸âƒ£ Limpiamos el texto:  
```python
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words("spanish"))

def limpiar_texto(texto):
    texto = texto.lower()
    texto = re.sub(r'[^\w\s]', '', texto)
    return " ".join([word for word in texto.split() if word not in stop_words])

df["texto_limpio"] = df["texto"].apply(limpiar_texto)
```

2ï¸âƒ£ Tokenizamos y vectorizamos:  
```python
tokenizer = Tokenizer(num_words=1000, oov_token="<OOV>")
tokenizer.fit_on_texts(df["texto_limpio"])
sequences = tokenizer.texts_to_sequences(df["texto_limpio"])
padded = pad_sequences(sequences, padding="post")
```

3ï¸âƒ£ Codificamos las etiquetas:  
```python
categorias = {"queja": 0, "pregunta": 1, "otro": 2}
df["categoria_num"] = df["categoria"].map(categorias)
```

---

### ğŸ“Œ **Paso 4: Dividir en entrenamiento y prueba**

```python
X_train, X_test, y_train, y_test = train_test_split(padded, df["categoria_num"], test_size=0.2, random_state=42)
```

---

### ğŸ“Œ **Paso 5: Definir la red neuronal**

```python
modelo = tf.keras.Sequential([
    tf.keras.layers.Embedding(1000, 16, input_length=padded.shape[1]),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(16, activation="relu"),
    tf.keras.layers.Dense(3, activation="softmax")
])
```

FunciÃ³n de pÃ©rdida que usaremos:  
`L = - Î£ [ yáµ¢ Â· log(Å·áµ¢) ]`

Compilamos el modelo:  
```python
modelo.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
```

---

### ğŸ“Œ **Paso 6: Entrenar la red neuronal**

```python
historial = modelo.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), verbose=2)
```

AquÃ­ ocurre:  
1ï¸âƒ£ PredicciÃ³n  
2ï¸âƒ£ CÃ¡lculo de error (entropÃ­a cruzada)  
3ï¸âƒ£ RetropropagaciÃ³n: `âˆ‚L / âˆ‚w`  
4ï¸âƒ£ Descenso de gradiente: `w = w - Î· * (âˆ‚L / âˆ‚w)`

---

### ğŸ“Œ **Paso 7: Evaluar el modelo**

```python
y_pred = np.argmax(modelo.predict(X_test), axis=1)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred, labels=[0,1,2], target_names=categorias.keys()))
```

Visualizamos la **matriz de confusiÃ³n**:  
```python
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap="Blues", xticklabels=categorias.keys(), yticklabels=categorias.keys())
```

---

### ğŸ“Œ **Paso 8: Probar en casos reales**

```python
def predecir_correo(texto):
    texto = limpiar_texto(texto)
    secuencia = tokenizer.texts_to_sequences([texto])
    secuencia_padded = pad_sequences(secuencia, maxlen=padded.shape[1])
    prediccion = modelo.predict(secuencia_padded)
    return list(categorias.keys())[np.argmax(prediccion)]

correo_nuevo = "Me llegÃ³ un producto daÃ±ado, Â¿quÃ© hago?"
print(predecir_correo(correo_nuevo))
```

---

### ğŸ“Œ **Paso 9: Visualizar el aprendizaje**

```python
plt.plot(historial.history["accuracy"], label="PrecisiÃ³n en entrenamiento")
plt.plot(historial.history["val_accuracy"], label="PrecisiÃ³n en validaciÃ³n")
plt.xlabel("Ã‰pocas")
plt.ylabel("PrecisiÃ³n")
plt.legend()
plt.show()

plt.plot(historial.history["loss"], label="PÃ©rdida en entrenamiento")
plt.plot(historial.history["val_loss"], label="PÃ©rdida en validaciÃ³n")
plt.xlabel("Ã‰pocas")
plt.ylabel("PÃ©rdida")
plt.legend()
plt.show()
```

---

## **Resultado final**

Hemos visto **cÃ³mo se entrena una red neuronal**, desde la teorÃ­a matemÃ¡tica hasta la prÃ¡ctica con cÃ³digo.

âœ… Aprendizaje supervisado  
âœ… RetropropagaciÃ³n y descenso de gradiente  
âœ… Predicciones y mejora continua

---

## **Â¿Quieres probarlo en tu empresa?**

ğŸ‘‰ Te puedo ayudar a implementar un modelo como este adaptado a tus **propios datos**.  
ğŸ‘‰ Agenda una llamada en [www.vialabsdigital.com](https://www.vialabsdigital.com) o responde a esta newsletter.

---

**RaÃºl JÃ¡uregui**  
*Consultor en IA & Machine Learning*  
[Mindful ML](https://mindfulml.vialabsdigital.com)

